{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c5a32e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
      "c:\\Users\\dream\\miniconda3\\envs\\ds\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import minari\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from PIL import Image\n",
    "from minari import DataCollector\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "# from torchvision.models import resnet50 # No longer using resnet50\n",
    "from torchvision.models import resnet18, ResNet18_Weights # Import ResNet-18\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy\n",
    "import d3rlpy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "from simulation import cancer\n",
    "from generate import simulate_blackwell_glynn\n",
    "from nsmm import nsmm_lag1, nsmm_lag1_cate\n",
    "from msm import MarginalStructuralModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "447b9f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dream\\projects\\Learning-Causal-Inference\\time_series\\simulation\\cancer.py:357: RuntimeWarning: overflow encountered in exp\n",
      "  if recovery_rvs[i, t] < np.exp(-cancer_volume[i, t] * tumour_cell_density):\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.random.seed(100)\n",
    "\n",
    "num_time_steps = 60  # 6 month followup\n",
    "num_patients = 1000\n",
    "\n",
    "simulation_params = cancer.get_confounding_params(num_patients, chemo_coeff=10.0, radio_coeff=10.0)\n",
    "simulation_params['window_size'] = 15\n",
    "\n",
    "outputs = cancer.simulate(simulation_params, num_time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b086268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your dataframe\n",
    "def prepare_data_for_outcome(df, drug_half_life = 1):\n",
    "    df = df.copy()\n",
    "    df['chemo_dosage'] = 0.0\n",
    "    df['previous_cancer_volume'] = df['cancer_volume']\n",
    "    for pid, group in df.groupby('Patient_ID'):\n",
    "        group = group.sort_values('Time_Point')\n",
    "        chemo_instant_dosage = group['chemo_instant_dosage']\n",
    "        previous_chemo_dose = group['chemo_instant_dosage'].shift(1)\n",
    "        previous_cancer_volume = group['cancer_volume'].shift(1)\n",
    "        chemo_dosages = cancer.get_chemo_dosage(chemo_instant_dosage, previous_chemo_dose, drug_half_life)\n",
    "        \n",
    "\n",
    "        df.loc[group.index, 'chemo_dosage'] = chemo_dosages\n",
    "        df.loc[group.index, 'previous_cancer_volume'] = previous_cancer_volume\n",
    "\n",
    "        df['termination'] = 0\n",
    "        df.loc[group.index[-1], 'termination'] = 1\n",
    "\n",
    "    return df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Example usage\n",
    "df = prepare_data_for_outcome(outputs)\n",
    "\n",
    "# drop some row since it need lag data\n",
    "n_time = int(len(df)/num_patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d89ff0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "state = df[['previous_cancer_volume']].to_numpy()\n",
    "action = df[['radio_dosage', 'chemo_instant_dosage']].to_numpy()\n",
    "reward = df[['previous_cancer_volume']].to_numpy()-df[['cancer_volume']].to_numpy()\n",
    "next_state = df[['cancer_volume']].to_numpy()\n",
    "terminations = df[['termination']].to_numpy()\n",
    "\n",
    "# Assuming all arrays are already defined and have the same length\n",
    "dataset = list(zip(state, action, reward, next_state, terminations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0269c81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim=2, hidden_dim=256):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)  # Q-value scalar\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        return self.net(x)  # returns Q-value for (s, a)\n",
    "\n",
    "\n",
    "class OfflineRL:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        gamma=0.99,\n",
    "        lr=1e-3,\n",
    "        batch_size=64,\n",
    "        device=\"cpu\"\n",
    "    ):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = 2  # fixed 2D binary action\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device(device)\n",
    "\n",
    "        self.q_net = QNetwork(state_dim).to(self.device)\n",
    "        self.target_q_net = QNetwork(state_dim).to(self.device)\n",
    "        self.target_q_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)\n",
    "\n",
    "        self.replay_buffer = []\n",
    "\n",
    "    def load_dataset(self, transitions):\n",
    "        \"\"\"\n",
    "        transitions: list of (state, action_vec[2], reward, next_state, done)\n",
    "        \"\"\"\n",
    "        self.replay_buffer = transitions\n",
    "\n",
    "    def sample_batch(self):\n",
    "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32, device=self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.float32, device=self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32, device=self.device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def all_binary_actions(self):\n",
    "        return torch.tensor([\n",
    "            [0, 0],\n",
    "            [0, 1],\n",
    "            [1, 0],\n",
    "            [1, 1]\n",
    "        ], dtype=torch.float32, device=self.device)\n",
    "\n",
    "    def train(self, epochs=100):\n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "            for _ in range(len(self.replay_buffer) // self.batch_size):\n",
    "                states, actions, rewards, next_states, dones = self.sample_batch()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    next_q_values = []\n",
    "                    all_actions = self.all_binary_actions()\n",
    "                    for a in all_actions:\n",
    "                        repeated_a = a.unsqueeze(0).expand(next_states.size(0), -1)\n",
    "                        q_val = self.target_q_net(next_states, repeated_a)\n",
    "                        next_q_values.append(q_val)\n",
    "                    next_q_values = torch.cat(next_q_values, dim=1)\n",
    "                    max_next_q = next_q_values.max(dim=1, keepdim=True)[0]\n",
    "\n",
    "                    target = rewards + self.gamma * (1 - dones) * max_next_q\n",
    "\n",
    "                q_pred = self.q_net(states, actions)\n",
    "\n",
    "                loss = nn.MSELoss()(q_pred, target)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                losses.append(loss.item())\n",
    "\n",
    "            self.update_target_network()\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {np.mean(losses):.4f}\")\n",
    "\n",
    "    def update_target_network(self, tau=0.005):\n",
    "        for param, target_param in zip(self.q_net.parameters(), self.target_q_net.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "    def predict_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        all_actions = self.all_binary_actions()\n",
    "        q_vals = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for a in all_actions:\n",
    "                a_input = a.unsqueeze(0).expand(state.size(0), -1)\n",
    "                q = self.q_net(state, a_input)\n",
    "                q_vals.append(q.item())\n",
    "\n",
    "        best_idx = np.argmax(q_vals)\n",
    "        return all_actions[best_idx].cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72e3d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dream\\miniconda3\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([3, 3, 1])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sac = OfflineRL(state.shape[1], action.shape[1], batch_size= 3)\n",
    "sac.load_dataset(dataset)  # Load the dataset into the replay buffer\n",
    "sac.train(epochs=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbb95de",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sac.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ae29129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cb29f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sac = d3rlpy.algos.DiscreteCQLConfig().create(device=\"cuda:0\")\n",
    "\n",
    "# train offline\n",
    "sac.fit(dataset, n_steps=50000)\n",
    "\n",
    "\n",
    "# --- Episode Runner Class (Interacting with Gym Environment) ---\n",
    "class EpisodeRunner:\n",
    "    def __init__(self, env, observation_transform=None):\n",
    "        self.env = env\n",
    "        self.observation_transform = observation_transform\n",
    "\n",
    "    def run_episode(self, initial_observation_raw, decision_policy_fn, model_estimator=None, use_random_policy=False):\n",
    "        episode_over = False\n",
    "        truncated = False\n",
    "        terminated = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        current_observation_raw = initial_observation_raw\n",
    "\n",
    "        while not (terminated or truncated):\n",
    "            current_observation_raw = np.expand_dims(current_observation_raw, axis=0)\n",
    "            if use_random_policy:\n",
    "                action = self.env.action_space.sample()\n",
    "            else:\n",
    "                assert model_estimator is not None, \"A model estimator must be provided for non-random actions\"\n",
    "                # Apply observation transform for the model\n",
    "                if self.observation_transform is not None:\n",
    "\n",
    "                    transformed_obs = self.observation_transform(current_observation_raw)\n",
    "                else: # Should not happen if model is used\n",
    "                    transformed_obs = torch.tensor(current_observation_raw, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "                action = decision_policy_fn(transformed_obs, model_estimator)\n",
    "            \n",
    "            next_observation_raw, reward, terminated, truncated, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            current_observation_raw = next_observation_raw\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                episode_over = True\n",
    "        \n",
    "        return total_reward\n",
    "\n",
    "def gym_observation_transform(observation_np_array): # Raw observation from env.step() or env.reset()\n",
    "\n",
    "    return np.transpose(observation_np_array, (0, 3, 1, 2))\n",
    "\n",
    "def decision_function_dl(obs_tensor, model_instance): # obs_tensor is already preprocessed\n",
    "\n",
    "    action = model_instance.predict(obs_tensor) # obs_tensor should be [1, C, H, W]\n",
    "\n",
    "    return action[0]\n",
    "\n",
    "# --- Environment Evaluation ---\n",
    "print(\"\\nStarting environment evaluation...\")\n",
    "total_reward_model_policy = 0\n",
    "total_reward_random_policy = 0\n",
    "\n",
    "\n",
    "env  = minari_dataset.recover_environment() \n",
    "\n",
    "\n",
    "\n",
    "print(f\"Running {N_EVAL_EPISODES} episodes for model policy and random policy...\")\n",
    "for i in tqdm.tqdm(range(N_EVAL_EPISODES), desc=\"Evaluating Policies\"):\n",
    "    # Model Policy\n",
    "\n",
    "    obs_model, info_model = env.reset()\n",
    "\n",
    "    env_model = deepcopy(env)\n",
    "    env_random = deepcopy(env)\n",
    "\n",
    "    runner_model = EpisodeRunner(env_model, observation_transform=gym_observation_transform)\n",
    "    reward_model = runner_model.run_episode(\n",
    "        obs_model,\n",
    "        decision_policy_fn=decision_function_dl,\n",
    "        model_estimator=sac,\n",
    "        use_random_policy=False\n",
    "    )\n",
    "    total_reward_model_policy += reward_model\n",
    "\n",
    "\n",
    "    runner_random = EpisodeRunner(env_random) # No transform needed for random\n",
    "    reward_random = runner_random.run_episode(\n",
    "        obs_model,\n",
    "        decision_policy_fn=None, # Not used for random\n",
    "        use_random_policy=True\n",
    "    )\n",
    "    total_reward_random_policy += reward_random\n",
    "\n",
    "avg_model_reward = total_reward_model_policy / N_EVAL_EPISODES if N_EVAL_EPISODES > 0 else 0\n",
    "avg_random_reward = total_reward_random_policy / N_EVAL_EPISODES if N_EVAL_EPISODES > 0 else 0\n",
    "\n",
    "print(f\"\\n--- Final Evaluation Results ({N_EVAL_EPISODES} episodes) ---\")\n",
    "print(f\"Average Total Reward (Model Policy): {avg_model_reward:.2f}\")\n",
    "print(f\"Average Total Reward (Random Policy): {avg_random_reward:.2f}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"Evaluation complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
